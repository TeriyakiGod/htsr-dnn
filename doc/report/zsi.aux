\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction - Kacper}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Our goal - Pawel [Done]}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Specific objectives}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Expected Outcomes}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision boundary - Kacper}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Weights and Biases - Pawel [Done]}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Initialization}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Initialization of random biases and weights using Xavier/Glorot technique}}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:initialized_weights}{{1}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Weighted sum of inputs}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The dot product of inputs and weights, representing the weighted sum of inputs}}{6}{}\protected@file@percent }
\newlabel{fig:weighted_sum}{{2}{6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Hidden layers - Kacper}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Activation functions - Pawel [Done]}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sigmoid function}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The derivative of the sigmoid function}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sigmoid function}}{8}{}\protected@file@percent }
\newlabel{fig:sigmoid_function}{{3}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The derivative of the sigmoid function}}{8}{}\protected@file@percent }
\newlabel{fig:sigmoid_derivativse_function}{{4}{8}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Cost function - Kacper}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Gradient descent and backpropagation - Pawel [Done]}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Gradients of the cost function}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Initialization a vector of zeros with the same length as the bias vector}}{10}{}\protected@file@percent }
\newlabel{fig:initialized_cost_gradient}{{5}{10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The \texttt  {apply\_gradients} method in \texttt  {Neural network} class updates the weights and biases for each layer.}}{10}{}\protected@file@percent }
\newlabel{fig:apply_gradients}{{6}{10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The \texttt  {apply\_gradients} method in \texttt  {Layer} class updates the weights and biases for a layer.}}{11}{}\protected@file@percent }
\newlabel{fig:apply_gradients_on_layer}{{7}{11}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Data loading and learning}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Data loading from mnist and creating a neural network}}{12}{}\protected@file@percent }
\newlabel{fig:data_load}{{8}{12}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Backpropagation}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Cost landscape - Kacper}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Learning algorithm - naive approach, calculus approach, digit recognition - Kacper}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Chain rule - Kacper}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Testing the network - Kacper}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Conclusion - Pawel}{17}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training a neural network using a simple form of gradient descent on the MNIST dataset.}}{18}{}\protected@file@percent }
\newlabel{fig:learn}{{9}{18}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The \texttt  {backpropagate} function is part of the neural network training process.}}{19}{}\protected@file@percent }
\newlabel{fig:backpropagation}{{10}{19}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The \texttt  {calculate\_gradients} method computes the gradients needed for the weight and bias adjustments.}}{19}{}\protected@file@percent }
\newlabel{fig:calculate_gradients}{{11}{19}{}{}{}}
\gdef \@abspage@last{20}
