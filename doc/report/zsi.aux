\relax 
\abx@aux@refcontext{nty/global//global/global}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Our goal}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Specific objectives}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Expected Outcomes}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision Boundary}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Definition}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Adaptability}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Optimization Impact}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Weights and Biases}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Initialization}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Initialization of random biases and weights using Xavier/Glorot technique}}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:initialized_weights}{{1}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Weighted sum of inputs}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The dot product of inputs and weights, representing the weighted sum of inputs}}{6}{}\protected@file@percent }
\newlabel{fig:weighted_sum}{{2}{6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Our model}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Activation functions}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sigmoid function}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The derivative of the sigmoid function}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sigmoid function}}{8}{}\protected@file@percent }
\newlabel{fig:sigmoid_function}{{3}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The derivative of the sigmoid function}}{8}{}\protected@file@percent }
\newlabel{fig:sigmoid_derivativse_function}{{4}{8}{}{}{}}
\abx@aux@cite{0}{algorithms}
\abx@aux@segm{0}{0}{algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {7}Gradient descent and backpropagation}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Gradients of the cost function}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Initialization a vector of zeros with the same length as the bias vector}}{9}{}\protected@file@percent }
\newlabel{fig:initialized_cost_gradient}{{5}{9}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The \texttt  {apply\_gradients} method in \texttt  {Neural network} class updates the weights and biases for each layer.}}{9}{}\protected@file@percent }
\newlabel{fig:apply_gradients}{{6}{9}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The \texttt  {apply\_gradients} method in \texttt  {Layer} class updates the weights and biases for a layer.}}{10}{}\protected@file@percent }
\newlabel{fig:apply_gradients_on_layer}{{7}{10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Data loading and learning}{10}{}\protected@file@percent }
\abx@aux@cite{0}{ffn_wiki}
\abx@aux@segm{0}{0}{ffn_wiki}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Data loading from mnist and creating a neural network}}{11}{}\protected@file@percent }
\newlabel{fig:data_load}{{8}{11}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Backpropagation}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Cost Landscape}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Definition}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Characteristics}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Impact on Optimization}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Optimization Strategies}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Learning algorithm - naive approach, calculus approach, digit recognition - Kacper}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Chain Rule}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Definition}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Application in Backpropagation}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Examples from our code}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Testing the network}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Conclusion}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training a neural network using a simple form of gradient descent on the MNIST dataset.}}{18}{}\protected@file@percent }
\newlabel{fig:learn}{{9}{18}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The \texttt  {backpropagate} function is part of the neural network training process.}}{19}{}\protected@file@percent }
\newlabel{fig:backpropagation}{{10}{19}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The \texttt  {calculate\_gradients} method computes the gradients needed for the weight and bias adjustments.}}{19}{}\protected@file@percent }
\newlabel{fig:calculate_gradients}{{11}{19}{}{}{}}
\abx@aux@read@bbl@mdfivesum{21B09E198E25E705D1F19E869E7AB2DC}
\abx@aux@defaultrefcontext{0}{algorithms}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{ffn_wiki}{nty/global//global/global}
\gdef \@abspage@last{20}
