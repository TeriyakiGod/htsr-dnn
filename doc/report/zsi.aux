\relax 
\abx@aux@refcontext{nty/global//global/global}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Our goal}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Specific objectives}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Expected Outcomes}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision Boundary}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Definition}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Adaptability}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Optimization Impact}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Weights and Biases}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Initialization}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Initialization of random biases and weights using Xavier/Glorot technique\relax }}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:initialized_weights}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Weighted sum of inputs}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The dot product of inputs and weights, representing the weighted sum of inputs\relax }}{6}{}\protected@file@percent }
\newlabel{fig:weighted_sum}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Our model}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architecture of our feedforward neural network model\relax }}{6}{}\protected@file@percent }
\newlabel{fig:model_architecture}{{3}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Activation functions}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sigmoid function}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The derivative of the sigmoid function}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sigmoid function\relax }}{8}{}\protected@file@percent }
\newlabel{fig:sigmoid_function}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The derivative of the sigmoid function\relax }}{8}{}\protected@file@percent }
\newlabel{fig:sigmoid_derivativse_function}{{5}{8}}
\abx@aux@cite{0}{algorithms}
\abx@aux@segm{0}{0}{algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {7}Gradient descent and backpropagation}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Gradients of the cost function}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Initialization a vector of zeros with the same length as the bias vector\relax }}{9}{}\protected@file@percent }
\newlabel{fig:initialized_cost_gradient}{{6}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The \texttt  {apply\_gradients} method in \texttt  {Neural network} class updates the weights and biases for each layer.\relax }}{9}{}\protected@file@percent }
\newlabel{fig:apply_gradients}{{7}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The \texttt  {apply\_gradients} method in \texttt  {Layer} class updates the weights and biases for a layer.\relax }}{10}{}\protected@file@percent }
\newlabel{fig:apply_gradients_on_layer}{{8}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Data loading and learning}{10}{}\protected@file@percent }
\abx@aux@cite{0}{ffn_wiki}
\abx@aux@segm{0}{0}{ffn_wiki}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Data loading from mnist and creating a neural network\relax }}{11}{}\protected@file@percent }
\newlabel{fig:data_load}{{9}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Backpropagation}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Cost Landscape}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Definition}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Characteristics}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Impact on Optimization}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Optimization Strategies}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Chain Rule}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Definition}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Application in Backpropagation}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Testing the network}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Test Dataset}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Evaluation Metrics}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Example Code}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Testing Results}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}User Interface}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusion}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Training a neural network using a simple form of gradient descent on the MNIST dataset.\relax }}{17}{}\protected@file@percent }
\newlabel{fig:learn}{{10}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The \texttt  {backpropagate} function is part of the neural network training process.\relax }}{18}{}\protected@file@percent }
\newlabel{fig:backpropagation}{{11}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The \texttt  {calculate\_gradients} method computes the gradients needed for the weight and bias adjustments.\relax }}{18}{}\protected@file@percent }
\newlabel{fig:calculate_gradients}{{12}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces User interface for testing the neural network.\relax }}{19}{}\protected@file@percent }
\newlabel{fig:ui}{{13}{19}}
\abx@aux@read@bbl@mdfivesum{D41D8CD98F00B204E9800998ECF8427E}
\abx@aux@read@bblrerun
\gdef \@abspage@last{20}
